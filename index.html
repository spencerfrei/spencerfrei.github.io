<!DOCTYPE html>
<html>
<!-- old rgb(255, 255, 208)  -->
  <head>
    <meta http-equiv="content-type" content="text/html; charset=windows-1252">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta content="Spencer Frei, Frei, machine learning, statistics, deep learning, optimization, generalization, Davis, UC Davis, Berkeley, UC Berkeley, UCLA, University of California, University of California Los Angeles"
      name="Keywords" lang="eng">
    <title> Spencer Frei </title>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="main.js"></script>
    <link href="https://fonts.googleapis.com/css?family=Fira+Sans:300,500,%20500italic"
      rel="stylesheet" type="text/css">
    <link href="main.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <div class="contact">
        <div class="contact__header">
          <div>
            <h1>Spencer Frei</h1>
	    Assistant Professor<br>
            <span style="color:#0D7680;"> <a href="https://statistics.ucdavis.edu/">Department of Statistics</a><br>
        <a href="https://www.ucdavis.edu/">University of California, Davis</a><br></span>
            <br>
            <div><strong>Email</strong>: sfrei@ucdavis.edu </div>
          </div>
          <img src="photo.png" alt="photo" style="width:150px; border-radius:8px"> </div>
        <nav>
          <ul>
		  <li class="link active-link" id="home-link"><b>Home</b></li>
		  <li class="link" id="activities-link"><b>News</b></li>
		  <li class="link" id="publications-link"><b>Publications</b></li>
          <li class="link" id="teaching-link"><b>Teaching</b></li> 
            <!--          <li class="link" id="tutor-link">Tutoring</li> -->
	    <!-- <li class="link" id="about-link"><b>Personal</b></li> -->
          </ul>
        </nav>
        <div class="content active" id="home">
          <p> I am an Assistant Professor of Statistics at UC Davis. I'm 
broadly interested in machine learning, statistics, and optimization, 
with a particular interest in understanding and improving deep learning<span>&#8212;</span>the technology underlying modern artificial intelligence.</p>

<style>
    ul {
        padding-left: 40px;   /* Or you can use 4ch for 4 character spaces */
    }
</style> 




          <p>Prior to joining UC Davis, I was a postdoctoral fellow at the <a href="https://simons.berkeley.edu/">Simons
              Institute for the Theory of Computing</a> at UC Berkeley, mentored 
            by <a href="https://www.stat.berkeley.edu/%7Ebartlett/">Peter
              Bartlett</a> and <a href="https://binyu.stat.berkeley.edu/">Bin
              Yu</a> as a part of the NSF/Simons <a href="https://deepfoundations.ai/">Collaboration
              on the Theoretical Foundations of Deep Learning</a>. I completed my PhD in Statistics at UCLA
            under the supervision of <a href="http://web.cs.ucla.edu/%7Eqgu/">Quanquan
              Gu</a> and <a href="http://www.stat.ucla.edu/%7Eywu/">Ying Nian
              Wu</a>.  <!-- Before that, I completed an MSc in Mathematics at the University of British Columbia and a BSc in Mathematics at McGill University. -->
More can be found on <a href="cv-sfrei.pdf">my CV.</a> </p>

          <div class="highlight publicationhighlight">NeurIPS 2023 Tutorial: <br>
              <a href="https://sml.inf.ethz.ch/gml23/neuripstut-blank.html">Reconsidering Overfitting in the Age of Overparameterized Models</a><br> 
              Spencer Frei, Vidya Muthukumar (Georgia Tech), and Fanny Yang (ETH Zurich).<br>
              <a href="https://nips.cc/virtual/2023/tutorial/73955">Video</a>, <a href="https://sml.inf.ethz.ch/assets/pdf/neuripstut_slides.pdf">Slides</a>, <a href="https://sml.inf.ethz.ch/gml23/neuripstut-blank.html">Webpage</a>, <a href="overfitting_tutorial.png">Photo</a>
          </div>


 <br>

 <h3>Selected recent works</h3>

			<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2306.09927">Trained transformers learn linear models in-context.</a> <br>	
			Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. <br>
			Journal of Machine Learning Research, to appear. <br>
		</div>


		<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2303.01462">Benign overfitting in linear classifiers and leaky ReLU networks from KKT conditions for margin maximization.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, and Nathan Srebro. <br>
			COLT 2023. <br>
		</div>

<!--		<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2303.01456">The double-edged sword of implicit bias: Generalization vs. robustness in ReLU networks.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, and Nathan Srebro. <br>
			Preprint, 2023. <br>
		</div> -->


		<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2210.07082">Implicit bias in leaky ReLU networks trained on high-dimensional data.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, Nathan Srebro, and Wei Hu. <br>
			ICLR 2023. <b>(Spotlight)</b> <br>
		</div>
	
	
		<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2202.05928">Benign overfitting without linearity: 
            Neural network classifiers trained by gradient descent for noisy linear data.</a> <br>
          Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett.<br>
	  COLT 2022.<br> 
		</div>



		<div class="highlight publicationhighlight"><a href="https://arxiv.org/abs/2106.13792">Proxy convexity: A unified
		    framework for the analysis of neural networks trained by gradient
		    descent.</a><br>
		  Spencer Frei and Quanquan Gu. <br>
		  NeurIPS 2021.<br>
		</div>





	   <br>
		 For a complete list of publications, click the Publications tab above. <br>
	
 
        </div>
        <div class="content" id="activities"> 
            * Paper on <a href="https://arxiv.org/abs/2306.09927">transformers and in-context learning</a> accepted for publication at JMLR. <br>
            * I will give a talk at the UC Davis Mathematics of Data and Decisions Seminar on May 21. <br>
            * I will give a talk at the UCLA Department of Statistics & Data Science Seminar on May 16. <br>
            * I will give a talk at the Sorbonne Universit&eacute;-Paris Diderot University Statistics seminar on April 30. <br>
            * I will give a talk at &Eacute;cole normale sup&eacute;rieure on April 29. <br>
            * Paper on <a href="https://arxiv.org/abs/2310.02541">benign overfitting and grokking in ReLU networks</a> was accepted at ICLR 2024. <br>
            * I will present the paper <a href="https://jmlr.org/papers/v24/22-1132.html">Random Feature Amplification</a> at ICLR 2024 as a part of the <a href="https://iclr.cc/public/JournalToConference">journal-to-conference track.</a> <br>
		<br><br>

	  	<details>
		<summary>Older news (click to expand)</summary>
		<br>

		<em><u>2023</u></em><br>
        * Two papers accepted at NeurIPS 2023 workshops: <a href="https://arxiv.org/abs/2306.09927">one</a> at <a href="https://sites.google.com/view/r0-fomo">Robustness of Foundation Models</a> (R0-FoMo), <a href="https://arxiv.org/abs/2310.02541">another</a> at <a href="https://sites.google.com/view/m3l-2023/home?authuser=0">Mathematics of Modern Machine Learning</a> (M3L). <br>
        * I will give a talk at Apple Machine Learning Research, Cupertino on November 29. <br>
        * New <a href="https://arxiv.org/abs/2310.02541">preprint</a> on benign overfitting and grokking with Zhiwei Xu, Yutong Wang, Gal Vardi, and Wei Hu. <br>
        * <a href="https://arxiv.org/abs/2303.01456">The Double-Edged Sword of Implicit Bias</a> was accepted at NeurIPS 2023. <br>
        * Random Feature Amplification was accepted for publication <a href="https://jmlr.org/papers/v24/22-1132.html">at JMLR.</a><br> 
        * I will present the tutorial on "Reconsidering Overfitting in the Age of Overparameterized Models" with Vidya Muthukumar and Fanny Yang at NeurIPS 2023. <br>
        * I will give a talk at the University of Basel Department of Mathematics and Computer Science seminar on November 9. <br> 
        * I will give a talk at the University of Cambridge Machine Learning Group on October 25. <br>
        * I will give a talk at Google DeepMind London on October 18. <br>
        * I will give a talk at the Imperial College London AI+X Seminar on October 17. <br>
        * I will give a talk at the University of Oxford Computational Statistics and Machine Learning Seminar on October 13. <br>
        * I will give a talk at Stanford University (Tengyu Ma's group) on September 11. <br>
        * I will give a talk at the Google Research in-context learning reading group on August 11th. <br>
        * New <a href="https://arxiv.org/abs/2308.03215">preprint</a> with Nikhil Ghosh, Wooseok Ha, and Bin Yu on learning a single-neuron autoencoder with SGD. <br>
        * I will be a member of the senior program committee for <a href="http://algorithmiclearningtheory.org/alt2024/">ALT 2024</a>.  <br>
        * New <a href="https://arxiv.org/abs/2306.09927">preprint</a> with Ruiqi Zhang and Peter Bartlett on in-context learning of linear models with transformers. <br>
        * <a href="https://arxiv.org/abs/2303.01462">Paper on benign overfitting in neural networks</a> was accepted at COLT 2023. <br>
		* I will be an Area Chair for <a href="https://neurips.cc/">NeurIPS 2023</a> in New Orleans. <br>
		* I will be speaking at the Youth in High Dimensions workshop in Trieste, Italy, from May 29-June 2.  Registration is available <a href="https://indico.ictp.it/event/10175">here</a>.<br>
		* I will be joining UC Davis as an Assistant Professor of Statistics in the fall. <br> 
		* Two new preprints with Gal Vardi, Peter Bartlett, and Nati Srebro: one on <a href="https://arxiv.org/abs/2303.01462">benign overfitting</a>, another on <a href="https://arxiv.org/abs/2303.01456">adversarial robustness.</a><br> 
		* I am on the program committee for <a href="https://www.learningtheory.org/colt2023/index.html">COLT 2023</a>. <br>
		* <a href="http://arxiv.org/abs/2210.07082">Paper on implicit bias in neural nets trained on high-dimensional data</a> was accepted for publication at ICLR 2023 as a spotlight presentation.<br>

        <br>
		<em><u>2022</u> </em><br> 
		* I will be speaking at the Symposium on Frontiers of Machine Learning and Artificial Intelligence at the University of Southern California on November 10th. <br>
		* <a href="http://arxiv.org/abs/2210.07082">New preprint</a> with Gal Vardi, Peter Bartlett, Nati Srebro, and Wei Hu on implicit bias in neural networks trained on high-dimensional data.  <br>
		* I have been selected as a 2022 Rising Star in Machine Learning by the University of Maryland. <br> 
		* I am giving a talk at the University of Alberta Statistics Seminar on October 26th. <br>
		* I am giving a talk at the <a href="https://memento.epfl.ch/event/external-flair-seminar-spencer-frei/">EPFL Fundamentals of Learning and Artificial Intelligence Seminar</a> on September 30th. <br>
		* I am a visiting scientist at EPFL in September and October, hosted by <a href="https://people.epfl.ch/emmanuel.abbe?lang=en">Emmanuel Abbe</a>.<br>
		* I am giving a talk at the Joint Statistical Meetings about <a href="https://arxiv.org/abs/2202.05928">benign overfitting without linearity</a>.<br>
		* <a href="https://arxiv.org/abs/2202.05928">Benign overfitting without linearity</a> was accepted at COLT 2022.<br>
		* I am an organizer for the <a href="https://simons.berkeley.edu/workshops/deep-learning-theory-workshop">Deep Learning Theory Summer School and Workshop</a>, to be held this summer at the Simons Institute. <br>
		* I will be speaking at the <a href="https://math.ethz.ch/news-and-events/events/research-seminars/daco-seminar.html?s=fs22#e_18438">ETH Zurich Data, Algorithms, Combinatorics, and Optimization Seminar</a> on June 7th. <br>
		* I will be a keynote speaker at the <a href="https://sgsu-uoft.github.io/ResearchDay2022/schedule.html">University of Toronto Statistics Research Day</a> on May 25th. <br>
		* I am giving a talk at Harvard University's Probabilitas Seminar on May 6th.<br>
		* Two <a href="https://arxiv.org/abs/2202.07626">recent</a> <a href="https://arxiv.org/abs/2202.05928">works</a> accepted at the Theory of Overparameterized Machine Learning 2022 <a href="https://topml.rice.edu/">workshop</a>, including <a href="https://arxiv.org/abs/2202.05928">one</a> as a contributed talk. <br>
		* I am giving a talk at the Microsoft Research ML Foundations Seminar on April 28th.<br>
		* I am giving a talk at the University of British Columbia (Christos Thrampoulidis's group) on April 8th.<br>
		* I am giving a talk at Columbia University (Daniel Hsu's group) on April 4th.<br>
		* I am giving a talk at Oxford University (Yee Whye Teh's group) on March 23rd.<br>
		* I am giving a talk at the NSF/Simons Mathematics of Deep Learning seminar on March 10th.<br>
		* I am giving a talk at the Google Algorithms Seminar on March 8th.<br>
		* I'm reviewing for the <a href="https://topml.rice.edu">Theory of Overparameterized Machine Learning 2022 workshop</a>. <br>
		* Two new preprints with Niladri Chatterji and Peter Bartlett: <a href="https://arxiv.org/abs/2202.05928">Benign Overfitting without Linearity</a> and <a href="https://arxiv.org/abs/2202.07626">Random Feature Amplification.</a> <br> 
          * Recent work on <a href="https://arxiv.org/abs/2106.13805">sample complexity of a self-training algorithm</a> accepted at AISTATS 2022. <br>
          <br>

          <em><u>2021</u> </em><br>
          * I am speaking at the <a href="https://simons.berkeley.edu/workshops/schedule/18696">Deep Learning Theory Symposium</a> at the Simons Institute on December 6th. <br>
          * My paper on <a href="https://arxiv.org/abs/2106.13792">proxy
            convexity as a framework for neural network optimization</a> was
          accepted at NeurIPS 2021. <br>
          * Two new preprints on arxiv: (1) <a href="https://arxiv.org/abs/2106.13792">Proxy
            convexity: a unified framework for the analysis of neural networks
            trained by gradient descent</a>, and (2) <a href="https://arxiv.org/abs/2106.13805">Self
            training converts weak learners to strong learners in mixture
            models.</a> <br>
          * I am reviewing for the ICML 2021 workshop <a href="https://sites.google.com/view/icml2021oppo/home">Overparameterization:
            Pitfalls and Opportunities</a> (ICMLOPPO2021). <br>
          * <a href="https://arxiv.org/abs/2010.00539">Three</a> <a href="https://arxiv.org/abs/2101.01152">recent</a>
          <a href="https://arxiv.org/abs/2104.09437">papers</a> accepted at
          ICML, including <a href="https://arxiv.org/abs/2010.00539">one</a> as
          a long talk.<br>
          * New <a href="https://arxiv.org/abs/2104.09437">preprint</a> on
          provable robustness of adversarial training for learning halfspaces
          with noise. <br>
          * I will be presenting recent <a href="https://arxiv.org/abs/2101.01152">work</a>
          at <a href="https://topml.rice.edu/">TOPML2021</a> as a lightning
          talk, and at the <a href="https://socalnlp.github.io/symp21/index.html">SoCal
            ML Symposium</a> as a spotlight talk. <br>
          * I'm giving a talk at the <a href="https://math.ethz.ch/sfs/news-and-events/young-data-science.html">ETH
            Zurich Young Data Science Researcher Seminar</a> on April 16th.<br>
          * I'm giving a talk at the Johns Hopkins University Machine Learning
          Seminar on April 2nd. <br>
          * I'm reviewing for the <a href="https://topml.rice.edu/">Theory of
            Overparameterized Machine Learning Workshop</a>. <br>
          * I'm giving a talk at the <a href="https://www.mis.mpg.de/montufar/seminars/math-ml-seminar.html">Max-Planck-Insitute
            (MPI) MiS Machine Learning Seminar</a> on March 11th. <br>
          * New <a href="https://arxiv.org/abs/2101.01152">preprint</a> showing
          SGD-trained neural networks of any width generalize in the presence of
          adversarial label noise. <br>
          <br>



          <em><u>2020</u> </em><br>
          * New <a href="https://arxiv.org/abs/2010.00539">preprint</a> on
          agnostic learning of halfspaces using gradient descent is now on
          arXiv. <br>
          * My <a href="https://arxiv.org/abs/2005.14426">single neuron paper</a>
          was accepted at NeurIPS 2020. <br>
          * I will be attending the <a href="https://www.ideal.northwestern.edu/special-quarters/fall-2020/">IDEAL
            Special Quarter on the Theory of Deep Learning</a> hosted by
          TTIC/Northwestern for the fall quarter. <br>
          * I've been awarded a <a href="https://grad.ucla.edu/funding/financial-aid/funding-for-continuing-students/dissertation-year-fellowship/">Dissertation
            Year Fellowship</a> by UCLA's Graduate Division.<br>
          * New <a href="https://arxiv.org/abs/2005.14426">preprint</a> on
          agnostic PAC learning of a single neuron using gradient descent is now
          on arXiv. <br>
          * New <a href="https://link.springer.com/article/10.1007/s00429-020-02083-w">paper</a>
          accepted at <i>Brain Structure and Function</i> from work with
          researchers at UCLA School of Medicine. <br>
          * I'll be (remotely) working at Amazon's <a href="https://www.amazon.science/conversational-ai-natural-language-processing">Alexa
            AI group</a> for the summer as a research intern, working on natural
          language understanding. <br>
          <br>
          <u><em>2019</em></u> <br>
          * My paper with Yuan Cao and Quanquan Gu, "Algorithm-dependent
          Generalization Bounds for Overparameterized Deep Residual Networks",
          was accepted at NeurIPS 2019 (<a href="https://arxiv.org/abs/1910.02934">arXiv
            version</a>, <a href="https://papers.nips.cc/paper/9619-algorithm-dependent-generalization-bounds-for-overparameterized-deep-residual-networks.pdf">NeurIPS
            version</a>).<br>
        </div>


        <div class="content" id="publications">


		
		<div class="publication"><a href="https://arxiv.org/abs/2310.02541">Benign overfitting and grokking in ReLU networks for XOR cluster data.</a> <br>	
            Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, and Wei Hu.<br>
			ICLR 2024. <br>
		</div>
		
		<div class="publication"><a href="https://arxiv.org/abs/2308.03215">The effect of SGD batch size on autoencoder learning: Sparsity, sharpness, and feature learning.</a> <br>	
			Nikhil Ghosh, Spencer Frei, Wooseok Ha, and Bin Yu. <br>
			Preprint, 2023. <br>
            <i>Preliminary version at NeurIPS 2022 workshop <a href="https://opt-ml.org/oldopt/opt22/papers.html">Optimization for Machine Learning.</a></i><br>
		</div>


			<div class="publication"><a href="https://arxiv.org/abs/2306.09927">Trained transformers learn linear models in-context.</a> <br>	
			Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. <br>
			Journal of Machine Learning Research, to appear. <br>
		</div>


		<div class="publication"><a href="https://arxiv.org/abs/2303.01456">The double-edged sword of implicit bias: Generalization vs. robustness in ReLU networks.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, and Nathan Srebro. <br>
			NeurIPS 2023. <br>
		</div>


			<div class="publication"><a href="https://arxiv.org/abs/2303.01462">Benign overfitting in linear classifiers and leaky ReLU networks from KKT conditions for margin maximization.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, and Nathan Srebro. <br>
			COLT 2023. <br>
		</div>


		<div class="publication"><a href="https://arxiv.org/abs/2210.07082">Implicit bias in leaky ReLU networks trained on high-dimensional data.</a> <br>	
			Spencer Frei*, Gal Vardi*, Peter L. Bartlett, Nathan Srebro, and Wei Hu. <br>
			ICLR 2023. <b>(Spotlight)</b> <br>
		</div>

		<div class="publication"><a href="https://arxiv.org/abs/2202.07626">Random feature amplification:
			Feature learning and generalization in neural networks.</a> <br>
          Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett.<br>
	  Journal of Machine Learning Research, 2023.<br> 
<!--	  <i>Also appeared at the Theory of Overparameterized Machine Learning
    (TOPML2022) <a href="https://topml.rice.edu/">workshop</a>.</i><br> -->

</div> 

		<div class="publication"><a href="https://arxiv.org/abs/2202.05928">Benign overfitting without linearity: 
            Neural network classifiers trained by gradient descent for noisy linear data.</a> <br>
          Spencer Frei, Niladri S. Chatterji, and Peter L. Bartlett.<br>
	  COLT 2022.<br> 
<!--		  <i>Also appeared at the Theory of Overparameterized Machine Learning
    (TOPML2022) <a href="https://topml.rice.edu/">workshop</a>.</i><br> -->

		</div>



		
	  <div class="publication">

		  <a href="https://arxiv.org/abs/2106.13805">Self-training converts weak
		    learners to strong learners in mixture models.</a> <br>
		  Spencer Frei*, Difan Zou*, Zixiang Chen*, and Quanquan Gu. <br>
		  AISTATS 2022. 
		  <!--<i>International Conference on Artificial Intelligence and Statistics (AISTATS)</i>, 2022. <br>-->
	  </div>
	  <div class="publication">
		  <a href="https://arxiv.org/abs/2106.13792">Proxy convexity: A unified
		    framework for the analysis of neural networks trained by gradient
		    descent.</a><br>
		  Spencer Frei and Quanquan Gu. <br>
		  NeurIPS 2021.
	  </div>
	  <div class="publication">
		  <a href="https://arxiv.org/abs/2104.09437">Provable robustness of
		    adversarial training for learning halfspaces with noise.</a> <br>
		  Difan Zou*, Spencer Frei*, and Quanquan Gu. <br>
		  ICML 2021.
	  </div>
	  <div class="publication">
		  <a href="https://arxiv.org/abs/2101.01152">Provable generalization of
		    SGD-trained neural networks of any width in the presence of
		    adversarial label noise.</a> <br>
		  Spencer Frei, Yuan Cao, and Quanquan Gu. <br>
		  ICML 2021.<br>
<!--		  <i>Also appeared at the Theory of Overparameterized Machine Learning
    (TOPML2021) <a href="https://topml.rice.edu/">workshop</a>.</i><br> -->
	  </div>
	  <div class="publication">
		  <a href="http://proceedings.mlr.press/v139/frei21a/frei21a-supp.pdf">Agnostic learning of
		    halfspaces with gradient descent via soft margins.</a> <br>
		  Spencer Frei, Yuan Cao, and Quanquan Gu. <br>
		  ICML 2021, <b>Oral (long talk).</b>
	  </div>
	  <div class="publication">
		  <a href="https://arxiv.org/abs/2005.14426">Agnostic learning of a
		    single neuron with gradient descent.</a> <br>
		  Spencer Frei, Yuan Cao, and Quanquan Gu. <br>
		  NeurIPS 2020.<br>
	  </div>

	  <div class="publication">
		  <a href="https://link.springer.com/article/10.1007/s00429-020-02083-w">Hemodynamic
		    latency is associated with reduced intelligence across the lifespan:
		    an fMRI DCM study of aging, cerebrovascular integrity, and cognitive
		    ability.</a> <br>
		  Ariana E. Anderson, Mirella Diaz-Santos, Spencer Frei <i>et
		    al.</i><br>
		  Brain Structure and Function, 2020. <br>
	  </div>


	  <div class="publication">
		  <a href="https://arxiv.org/abs/1910.02934">Algorithm-dependent
		    generalization bounds for overparameterized deep residual networks.</a>
		  <br>
		  Spencer Frei, Yuan Cao, and Quanquan Gu. <br>
		  NeurIPS 2019.
	  </div>

	  <div class="publication">
		  <a href="https://projecteuclid.org/euclid.ejp/1473188083">A lower
		    bound for <span class="math">$p_c$</span> in range-<span class="math">$R$</span>
		    bond percolation in two and three dimensions.</a> <br>
		  Spencer Frei and Edwin Perkins. <br>
		  Electronic Journal of Probability, 2016. 
	  </div>

	  <div class="publication">
		  <a href="https://link.springer.com/article/10.1007/s10665-013-9655-4">On
		    thermal resistance in concentric residential geothermal heat
		    exchangers.</a><br>
		  Spencer Frei, Kathryn Lockwood, Greg Stewart, Justin Boyer, and
		  Burt S. Tilley.<br>
		  Journal of Engineering Mathematics, 2014. 
	  </div>
	  <br>
	  
          * denotes equal contribution. <br>
          <br>
        </div>

        <div class="content" id="teaching">
In Winter 2024, I will be the instructor for two courses at UC Davis:
<ul>
    <li><a href="teaching/sta35b/index.html" target="_blank">STA 035B: Statistical Data Science II.</a></li>
  <li><a href="teaching/foundations/index.html" target="_blank">STA 250: Theoretical Foundations of Modern AI.</a></li>
</ul>
I will also be the organizer of the Winter 2024 seminar for the statistics department (STA 290). 



      </div>

      </div>
    </div>
  </body>
</html>
