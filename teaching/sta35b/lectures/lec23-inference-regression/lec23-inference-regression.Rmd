---
title: "Inference for comparing many means, and inference for linear regression"
subtitle: "<br><br> STA35B: Statistical Data Science 2"
author: "Spencer Frei <br> Figures taken from [IMS], Cetinkaya-Rundel and Hardin text"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    # lib_dir: libs
    nature:
      ratio: "16:9"
      # highlightLines: true
      # highlightStyle: solarized-light
      countIncrementalSlides: false
---
  


  
```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(unvotes)
library(knitr)
library(broom)
library(patchwork)
library(ggpubr)
library(scales) # label_dollar 
library(quantreg) # rq
library(kableExtra)
library(openintro)
library(infer)
library(gghighlight)
library(janitor)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

knitr::opts_chunk$set(comment = NA) # makes it so the ## doesnt appear in output for chunks

source("../_common.R")

library(xaringanthemer)
# style_mono_light(base_color = "#23395b")
style_mono_accent(base_color = "#23395b")
```
.pull-left[
#### F distribution
* For comparing many means, we use $F$ distribution/statistic
* **Goal**: assess whether or not the variability we see in sample means is so large that it is unlikely to be due to random chance
* To do this, we compute the **sum of squares between groups**.
* For each group, we can calculate a sample mean $\bar x_i$, $i=1, ..., k$ if you have $k$ groups (each group has $n_i$ people)
* We can also calculate the sample mean across *all* observations, $\bar x$

$$\begin{align*}\text{sum of squares btwn groups}&=SSG \newline\\
&= \sum_{j=1}^k n_j (\bar x_j - \bar x)^2\end{align*}$$

]

.pull-right[
* We want to compare this to the total variability of all samples to the total mean across all samples using **sum of squared errors**

$$\begin{align*}SSE&= \sum_{i=1}^n ( x_i - \bar x)^2\end{align*}$$
* The **mean square between groups** is normalized version of SSG, and **mean squared errors** is normalized version of SSE:

$$\begin{align*} MSG &= \frac{1}{\mathsf{df}_G} SSG = \frac{1}{k-1} SSG,\newline MSE&=\frac{1}{\mathsf{df}_E} SSE =\frac{1}{n-k} SSE.\end{align*}$$
* The F-statistic is ratio of these two quantities:
$$F = \frac{MSG}{MSE}$$
* Under null hypothesis that all means are the same, and under conditions, F has $F$ distribution with $df_1=k-1$, $df_2=n-k$. 
]

---

#### ANOVA tables from R
* Let's again look at the MLB data (note we are using a modified version of the `mlb_players_18` dataset)
```{r}
#| echo: false
mlb_players_18 <- mlb_players_18 %>%
  filter(
    AB >= 100, 
    !position %in% c("P", "DH")
  ) %>%
  mutate(
    position = case_when(
      position %in% c("LF", "CF", "RF")       ~ "OF",
      position %in% c("1B", "2B", "3B", "SS") ~ "IF",
      TRUE                                    ~ position
    ),
    position = fct_relevel(position, "OF", "IF", "C")
  )
mlb_players_18 %>%
  arrange(name) %>% 
  select(name, team, position, OBP) %>%
  slice_head(n = 4) %>%
  kbl(linesep = "", booktabs = TRUE, align = "lllrrrrrr") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"), full_width = TRUE)
```

* We checked last time that all of the conditions needed for applying F statistic hold, except possibly independence which we couldn't check (let's assume it does hold)

* Output of ANOVA and lm in R:
```{r}
mod <- lm(OBP ~ position, data = mlb_players_18)
anova(mod) %>%
  tidy() %>% kable() 
```

---
#### Understanding the ANOVA output in R
```{r}
#| echo: false
mod <- lm(OBP ~ position, data = mlb_players_18)
anova(mod) %>%
  tidy() %>% kable() 
```

* You can derive this entire tibble by using just `df` and `sumsq` (and `pf()` to calc p-value from the statistic)
```{r, echo = FALSE}
tb <- anova(mod) %>% tidy()
tb$meansq[1] <- "X"
tb$meansq[2] <- "X"
tb$statistic[2] <- "X"
tb$statistic[1] <- "X"
tb$p.value[1] <- "X"
tb %>% kable()
```
* For `position` = independent variable, `df` $=k-1$ if $k$ groups; `sumsq` <-> SSG, `meansq` <-> MSG
* For `Residuals`, `df`$=n-k$; `sumsq` <-> SSE, `meansq` <-> MSE
* `statistics` is the F statistic, $F = MSG/MSE$



---
#### Understanding the ANOVA output in R
```{r}
#| echo: false
mod <- lm(OBP ~ position, data = mlb_players_18)
anova(mod) %>%
  tidy() %>% kable() 
```

* You can derive this entire tibble by using just `df` and `sumsq` (and `pf()` to calc p-value from the statistic)
```{r, echo = FALSE}
tb <- anova(mod) %>% tidy()
tb$statistic[2] <- "X"
tb$statistic[1] <- "X"
tb$p.value[1] <- "X"
tb %>% kable()
```
* For `position` = independent variable, `df` $=k-1$ if $k$ groups; `sumsq` <-> SSG, `meansq` <-> MSG
* For `Residuals`, `df`$=n-k$; `sumsq` <-> SSE, `meansq` <-> MSE
* `statistics` is the F statistic, $F = MSG/MSE$

.pull-left[ 
* For `position`, `meansq` $\approx 0.016 / 2 = 0.008$
* For `Residuals`, `meansq` $\approx 0.674 / 426 = 0.00158$
]

---
#### Understanding the ANOVA output in R
```{r}
#| echo: false
mod <- lm(OBP ~ position, data = mlb_players_18)
anova(mod) %>%
  tidy() %>% kable() 
```

* You can derive this entire tibble by using just `df` and `sumsq` (and `pf()` to calc p-value from the statistic)
```{r, echo = FALSE}
tb <- anova(mod) %>% tidy()
tb$p.value[1] <- "X"
tb %>% kable()
```
* For `position` = independent variable, `df` $=k-1$ if $k$ groups; `sumsq` <-> SSG, `meansq` <-> MSG
* For `Residuals`, `df`$=n-k$; `sumsq` <-> SSE, `meansq` <-> MSE
* `statistics` is the F statistic, $F = MSG/MSE$

.pull-left[ 
* For `position`, `meansq` $\approx 0.016 / 2 = 0.008$
* For `Residuals`, `meansq` $\approx 0.674 / 426 = 0.00158$
* `statistic`: MSG/MSE $\approx 0.008/0.00158 \approx 5.07$
]

---
#### Understanding the ANOVA output in R
```{r}
#| echo: false
mod <- lm(OBP ~ position, data = mlb_players_18)
anova(mod) %>%
  tidy() %>% kable() 
```

* You can derive this entire tibble by using just `df` and `sumsq` (and `pf()` to calc p-value from the statistic)
```{r, echo = FALSE}
tb <- anova(mod) %>% tidy()
tb %>% kable()
```
* For `position` = independent variable, `df` $=k-1$ if $k$ groups; `sumsq` <-> SSG, `meansq` <-> MSG
* For `Residuals`, `df` $=n-k$; `sumsq` <-> SSE, `meansq` <-> MSE
* `statistics` is the F statistic, $F = MSG/MSE$

.pull-left[ 
* For `position`, `meansq` $\approx 0.016 / 2 = 0.008$
* For `Residuals`, `meansq` $\approx 0.674 / 426 = 0.00158$
]

.pull-right[
* `statistic`: MSG/MSE $\approx 0.008/0.00158 \approx 5.07$
* For `p.value`, can use `pf`:
```{r}
1 - pf(5.08, df1 = 2, df2 = 426)
```

]

---

.pull-left[
### Inference for linear regression with a single variable 
* Let's think about linear regression again
* We'll consider setting where we have chain sandwich stores which spent different amounts on advertising and then the amount of revenue they received
* Let's imagine we had access to **all** data, for all chain sandwich stores and amount spent on advertising, and it looked like this 


```{r}
#| echo: false
#| message: false
#| warning: false
#| out.width: 80%
set.seed(4747)
popsize <- 1000
ad <- rnorm(popsize, 4, 1)
rev <- 12 + 4.7 * ad + rnorm(popsize, 0, 8)
sandwich <- tibble(ad, rev)

ggplot(sandwich, aes(x = ad, y = rev)) +
  geom_point(alpha = 0.5, size = 2) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  scale_x_continuous(labels = label_dollar(suffix = "K")) +
  scale_y_continuous(labels = label_dollar(suffix = "K")) +
  labs(
    x = "Amount spent on advertising",
    y = "Revenue",
    title = "Chain sandwich store",
    subtitle = "Hypothetical population"
  ) +
  coord_cartesian(ylim = c(0, 65))
```

]

.pull-right[

The population model is: $$y = \beta_0 + \beta_1 x + \varepsilon.$$

If we had full information of all data in the true population, model would be: $$\texttt{expected revenue} = 11.23 + 4.8 \times \texttt{advertising}.$$
* For every \$1,000 spent on advertising, revenue increases by \$4,800 on average
* If we spent nothing on advertising, we expect \$11,230 revenue on average 
* However, unrealistic to expect that we can have knowledge of **all** data---more realistically, we have knowledge of a sample. 
]

---

.pull-left[
* When we take samples, our estimates for the slope of the line in linear regression change from sample to sample


```{r}
#| echo: false
set.seed(470)
sandwich2 <- sandwich %>%
  sample_n(size = 20)
sandwich3 <- sandwich %>%
  sample_n(size = 20)
sandwich_many <- sandwich %>%
  rep_sample_n(size = 20, replace = FALSE, reps = 50)
```

```{r}
#| label: fig-sand-samp1
#| echo: false
#| message: false
#| warning: false
ggplot(sandwich2, aes(x = ad, y = rev)) +
  geom_point(size = 3, fill = IMSCOL["green", "full"], color = "#FFFFFF", shape = 22) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE, color = IMSCOL["green", "full"]) +
  scale_x_continuous(labels = label_dollar(suffix = "K")) +
  scale_y_continuous(labels = label_dollar(suffix = "K")) +
  labs(
    x = "Amount spent on advertising",
    y = "Revenue",
    title = "Chain sandwich store",
    subtitle = "Random sample of 20 stores"
  ) +
  coord_cartesian(ylim = c(0, 65))
```


]

--

.pull-right[
```{r}
#| label: fig-sand-samp2
#| echo: false 
#| message: false
#| warning: false
#| out.width: 80%
ggplot(sandwich3, aes(x = ad, y = rev)) +
  geom_point(size = 3, fill = IMSCOL["gray", "full"], color = "#FFFFFF", shape = 23) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE, color = IMSCOL["gray", "full"]) +
  scale_x_continuous(labels = label_dollar(suffix = "K")) +
  scale_y_continuous(labels = label_dollar(suffix = "K")) +
  labs(
    x = "Amount spent on advertising",
    y = "Revenue",
    title = "Chain sandwich store",
    subtitle = "Random sample of 20 stores"
  ) +
  coord_cartesian(ylim = c(0, 65))
```

If we did this on many different samples of 20 stores, we would get varied lines:


```{r}
#| label: fig-slopes
#| echo: false
#| warning: false
#| message: false
#| out.width: 80%

ggplot() +
  geom_smooth(
    data = sandwich_many, aes(x = ad, y = rev, group = replicate),
    method = "lm", se = FALSE, fullrange = TRUE, 
    color = IMSCOL["gray", "f2"], size = 0.5
  ) +
  geom_smooth(
    data = sandwich, aes(x = ad, y = rev), method = "lm", se = FALSE,
    fullrange = TRUE, color = IMSCOL["red", "full"]
  ) +
  scale_x_continuous(labels = label_dollar(suffix = "K")) +
  scale_y_continuous(labels = label_dollar(suffix = "K")) +
  labs(
    x = "Amount spent on advertising",
    y = "Revenue",
    title = "Chain sandwich store",
    subtitle = "Many random samples of 20 stores"
  ) +
  coord_cartesian(ylim = c(0, 65))
```
]

---
.pull-left[
* The relevant hypotheses for a linear model are about whether or not the population slope parameter is zero.
  - $H_0$: $\beta_1 = 0$, no linear relationship between response and independent variable
  - $H_A$: $\beta_1 \neq 0$, some linear relationship between response and independent variable

* We can perform similar analyses that we used before:
  - Randomization test
  - Bootstrap confidence interval
  - Mathematical approach

]

--

.pull-right[
#### Randomization test
* In the randomization test, we randomly permute the responses 
* Then any structure which existed in original dataset (for independent variable --> response) disappears
* This gives us a baseline for what types of variability we should expect under null hypothesis
* E.g. let's consider **births14** dataset as before, which has baby birth weight and number of weeks of gestation


```{r}
#| label: fig-permweightScatter
#| echo: false
#| fig-asp: 0.5
set.seed(4747)
births14_100 <- births14 %>%
  drop_na() %>%
  sample_n(100)
p1 <- ggplot(births14_100) +
  geom_point(aes(x = weeks, y = weight)) +
  labs(
    x = "Length of gestation (weeks)",
    y = "Weight of baby (pounds)",
    title = "Original data"
  )

p2 <- ggplot(births14_100) +
  geom_point(aes(x = weeks, y = sample(weight))) +
  labs(
    x = "Length of gestation (weeks)",
    y = "Permuted weight of baby (pounds)",
    title = "Permuted data"
  )

p1 + p2
```
]

---

.pull-left[
* When we do repeated randomizations, we see what types of slopes we would get when there is no relationship between the response and the independent variable.


```{r}
#| label: fig-permweekslm
#| echo: false
#| fig-asp: 0.5
#| warning: false
#| message: false
set.seed(470)
p1 <- ggplot(births14_100, aes(x = weeks, y = sample(weight))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  labs(
    y = "Permuted weight of baby (pounds)",
    x = "Length of gestation (weeks)",
    title = "First permutation of \nweight"
  )

p2 <- ggplot(births14_100, aes(x = weeks, y = sample(weight))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  labs(
    y = "Permuted weight of baby (pounds)",
    x = "Length of gestation (weeks)",
    title = "Second permutation of \nweight"
  )

p1 + p2
```
]

--

.pull-right[

```{r}
#| label: fig-nulldistBirths
#| echo: false
#| warning: false
#| message: false

perm_slope <- births14_100 %>%
  specify(weight ~ weeks) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")

obs_slope <- births14_100 %>%
  specify(weight ~ weeks) %>%
  calculate(stat = "slope") %>%
  pull()

ggplot(data = perm_slope, aes(x = stat)) +
  geom_histogram() +
  geom_vline(xintercept = obs_slope, color = IMSCOL["red", "full"]) +
  labs(x = "Randomly generated slopes", y = "Count")
```


* If there were no linear relationship between `weight` and `weeks`, the natural variability of the slopes would produce estimates between approximately -0.15 and +0.15.
 * We reject the null hypothesis.
* We believe that the slope observed on the original data is not just due to natural variability---rather, we believe there is a linear relationship between `weight` of baby and `weeks` gestation 

]

---

.pull-left[
#### Bootstrap for confidence interval for the slope
* Let's now consider looking at predicting weight of the baby as a function of mother's age
* When we run the following code, we get
```{r}
model <- lm(weight ~ mage, data = births14_100) 
```


```{r}
#| echo: false
set.seed(4747)
births4 <- births14_100 %>%
  sample_n(size = 100, replace = TRUE)
births5 <- births14_100 %>%
  sample_n(size = 100, replace = TRUE)
births_many_BS <- births14_100 %>%
  rep_sample_n(size = 100, replace = TRUE, reps = 50)
```

```{r}
#| label: tbl-ls-births-mage
#| echo: false
lm(weight ~ mage, data = births14_100) %>%
  tidy() %>%
  mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) %>%
  kbl(linesep = "", booktabs = TRUE, 
      digits = 2, align = "lrrrr") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position")) %>%
  column_spec(1, width = "10em", monospace = TRUE) %>%
  column_spec(2:5, width = "5em")
```

* For bootstrap-, we will take bootstrap samples of the data (sampling with replacement) 
* We will sometimes get the same sample in the resulting bootstrap, sometimes will not, sometimes get it twice
]

--

.pull-right[

```{r}
#| label: fig-birth2BS
#| echo: false
#| message: false
#| warning: false
#| out.width: 85%
p1 <- ggplot(births14_100) +
  geom_point(aes(x = mage, y = weight), alpha = 0.4) +
  geom_smooth(aes(x = mage, y = weight),
    method = "lm", se = FALSE,
    fullrange = TRUE
  ) +
  xlab("mother's age") +
  ylab("weight of baby") +
  ggtitle("Original births data.") +
  geom_point(x = 27, y = 9.7, color = IMSCOL["green", "full"], pch = 1, size = 8) +
  geom_point(x = 21, y = 5.7, color = IMSCOL["red", "full"], pch = 1, size = 8) +
  geom_point(x = 39, y = 9, pch = 1, size = 8)

p2 <- ggplot(births5) +
  geom_point(aes(x = mage, y = weight), alpha = 0.4) +
  geom_smooth(aes(x = mage, y = weight),
    method = "lm", se = FALSE,
    fullrange = TRUE
  ) +
  xlab("mother's age") +
  ylab("weight of baby") +
  ggtitle("Bootstrap sample from \nthe births data.") +
  geom_point(x = 27, y = 9.7, color = IMSCOL["green", "full"], pch = 1, size = 8) +
  geom_point(x = 21, y = 5.7, color = IMSCOL["red", "full"], pch = 1, size = 8) +
  geom_point(x = 39, y = 9, pch = 1, size = 8)

p1
p2
```

]

---

.pull-left[
* For each bootstrap sample, we compute a linear model on the data
* We then plot a histogram of all of the different slopes we get
* This gives us a sense of the variability of the slopes across random samples
* Suppose all of the bootstrap slopes are given in the vector `slopes`
* The following code would provide allow for calculating the bootstrap 95% confidence interval
```{r, eval=FALSE}
quantile(x, probs = c(0.025, 0.975))
```

]

--

.pull-right[


```{r}
#| label: fig-mageBSslopes
#| echo: false
#| warning: false
#| message: false
#| out.width: 85%

set.seed(47)
births_many_BS_1000 <- births14_100 %>%
  rep_sample_n(size = 100, replace = TRUE, reps = 1000)

births_many_lm_BS <- births_many_BS_1000 %>%
  group_by(replicate) %>%
  do(tidy(lm(weight ~ mage, data = .))) %>%
  filter(term == "mage")

lower <- round(quantile(births_many_lm_BS$estimate, 0.025), 3)
upper <- round(quantile(births_many_lm_BS$estimate, 0.975), 3)

ggplot(births_many_lm_BS, aes(x = estimate)) +
  geom_histogram() +
  annotate("segment", x = lower, xend = lower, y = 0, yend = 25, linetype = "dashed") +
  annotate("segment", x = upper, xend = upper, y = 0, yend = 25, linetype = "dashed") +
  annotate("text", x = lower, y = 35, label = "2.5 percentile\n-0.01", size = 5) +
  annotate("text", x = upper, y = 35, label = "97.5 percentile\n0.081", size = 5) +
  labs(
    x = "Bootstrapped values of the slope modeling weight on mother's age",
    y = NULL
  ) +
  theme(axis.text.y = element_blank())
```
* From this we say that we are 95% confident that, for the model of weight of a baby described by mother's age, a one unit increase in mother's age will be associated with an increase in predicted average baby weight of between -0.01 and 0.081 pounds
* This includes 0, so it is possible mother's age does not have meaningful linear relationship with baby's weight 
]


---

.pull-left[
### Mathematical model for testing a slope
* Under certain conditions, can use the $t$-distribution to test and estimate the slope parameter in linear regression.  Conditions:

-   **Linearity.** The data should show a linear trend.

-   **Independent observations.** Be cautious about applying regression to data that are sequential observations in time, e.g. stock prices/day

-   **Nearly normal residuals.** Generally, the residuals should be nearly normal.   When this condition is found to be unreasonable, it is often because of outliers or concerns about influential points
    
-   **Constant or equal variability.** The variability of points around the least squares line remains roughly constant.


]

--

.pull-right[

```{r}
#| out.width: 100%
#| fig.width: 10.0
#| fig-asp: 0.5
#| echo: false
#| warning: false
#| message: false
par_og <- par(no.readonly = TRUE) # save original par
par(mar = rep(1.2, 4))

source("helper-makeTubeAdv.R")
pch <- 20
cex <- 1.75
col <- IMSCOL["blue", "f2"]

layout(
  matrix(1:8, 2),
  rep(1, 4),
  c(2, 1)
)


these <- simulated_scatter$group == 20
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
  axes = FALSE,
  pch = pch,
  cex = cex,
  col = "#00000000"
)
box()
mtext("x", side=1, line=0.2)
mtext("y", side=2, line=0.2)

makeTube(x, y,
  type = "quad",
  addLine = FALSE,
  col =  IMSCOL["lgray", "f2"]
)
points(x, y,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"]
)
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
  xlab = "", ylab = "",
  axes = FALSE,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"],
  ylim = yR
)
abline(h = 0, lty = 2)
box()
mtext("Predicted y", side=1, line=0.2)
mtext("Residual", side=2, line=0.1)

these <- simulated_scatter$group == 21
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
  axes = FALSE,
  pch = pch,
  cex = cex,
  col = "#00000000"
)
box()
mtext("x", side=1, line=0.2)
mtext("y", side=2, line=0.2)

makeTube(x, y,
  addLine = FALSE,
  col =  IMSCOL["lgray", "f2"]
)
points(x, y,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"]
)
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
  xlab = "", ylab = "",
  axes = FALSE,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"],
  ylim = yR
)
abline(h = 0, lty = 2)
box()
mtext("Predicted y", side=1, line=0.2)
mtext("Residual", side=2, line=0.1)

these <- simulated_scatter$group == 22
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
  axes = FALSE,
  pch = pch,
  cex = cex,
  col = "#00000000"
)
box()
mtext("x", side=1, line=0.2)
mtext("y", side=2, line=0.2)

makeTubeAdv(x, y,
  type = "l",
  variance = "l",
  bw = 0.03,
  Z = 1.7,
  col =  IMSCOL["lgray", "f2"]
)
points(x, y,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"]
)
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
  axes = FALSE,
  xlab = "", ylab = "",
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"],
  ylim = yR
)
abline(h = 0, lty = 2)
box()
mtext("Predicted y", side=1, line=0.2)
mtext("Residual", side=2, line=0.1)

these <- simulated_scatter$group == 23
x <- simulated_scatter$x[these]
y <- simulated_scatter$y[these]
plot(x, y,
  axes = FALSE,
  pch = pch,
  cex = cex,
  col = "#00000000"
)
box()
mtext("x", side=1, line=0.2)
mtext("y", side=2, line=0.2)
makeTube(x, y,
  addLine = FALSE,
  col =  IMSCOL["lgray", "f2"]
)
points(x, y,
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"]
)
g <- lm(y ~ x)
abline(g)
yR <- range(g$residuals)
yR <- yR + c(-1, 1) * diff(yR) / 10
plot(x, g$residuals,
  axes = FALSE,
  xlab = "", ylab = "",
  pch = pch,
  cex = cex,
  col =  IMSCOL["blue", "f1"],
  ylim = yR
)
abline(h = 0, lty = 2)
box()
mtext("Predicted y", side=1, line=0.2)
mtext("Residual", side=2, line=0.1)
makeTubeAdv(x, y, col =  IMSCOL["lgray", "f2"])
#par(par_og) # restore original par
```


]

---

.pull-left[
### Example
* Let's consider dataset that has info on elections, with unemployment rate and election outcomes in midterm elections
* The question is whether higher unmployment rate entails worse performance for the current President's party
* `midterms_house` data in *openintro* 
```{r, echo=FALSE}
str(midterms_house)
```

]

--

.pull-right[
* Let's build a linear model for the change in house seats for a president's party as a function of unemployment rate
* Let's restrict to election years OUTSIDE of the Great Depression (i.e., not year 1935 or 1939)
* First check that there are no substantial outliers, no clear nonlinearity

```{r}
#| label: fig-unemploymentAndChangeInHouse
#| message: false
#| warning: false
#| out.width: 65%
midterms_house %>%
  filter(!(year %in% c(1935, 1939))) %>%
  ggplot(aes(x = unemp, y = house_change)) +
  geom_point(aes(color = party, shape = party), size = 3, alpha = 0.7) +
  theme(legend.position = c(0.8, 0.8), legend.background = element_rect(color = "white")
  )
```
]

---
Let's investigate the residuals:

.pull-left[

```{r}
#| echo: false
#| warning: false
#| message: false
midterms_house %>%
  filter(!(year %in% c(1935, 1939))) %>%
  ggplot(aes(x = unemp, y = house_change)) +
  geom_point(aes(color = party, shape = party), size = 3, alpha = 0.7) +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1), limits = c(3.5, 12.1), breaks = c(4, 8, 12)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1), limits = c(-31, 11), breaks = c(-30, -20, -10, 0, 10)) +
  labs(
    x = "Percent unemployed", 
    y = "Percent change in House seats\nfor the President's party",
    color = "Party", shape = "Party"
    ) +
  theme(
    legend.position = c(0.8, 0.8),
    legend.background = element_rect(color = "white")
  )
```
]

.pull-right[
```{r}
#| echo: false
#| warning: false
#| message: false
mod <-  lm(house_change ~ unemp, data = midterms_house %>% filter(!(year %in% c(1935, 1939))))
modf <- fortify(mod)
ggplot(modf, aes(x = .fitted, y = .resid)) + geom_point()
```
* No clear problems with the residuals
* Does appear the linear model might not be the best fit, but let's investigate 
]
---


.pull-left[ 
* Let's build the linear model for the non-Great Depression data
```{r}
not_gd_data <- midterms_house %>% 
  filter(! ( year %in% c(1935, 1939)))

lm(house_change ~ unemp, data = not_gd_data) %>% tidy()
```
* From this we see the following predictive model:


$$
\begin{aligned}
&\texttt{Percent change House seats for President's party}  \newline \\
&\qquad\qquad= -7.36 - 0.89 \times \texttt{(unemployment rate)}
\end{aligned}
$$
* Predicts greater unemployment rate implies worse performance in House midterm election
* Is there significant evidence that the "true" linear model has a negative slope?
]

--

.pull-right[
* We can frame this as hypothesis test:
  - $H_0:\ \beta_1 = 0$, true linear model has slope zero
  - $H_A:\ \beta_1 \neq 0$, true linear model has slope different than zero; unmployment rate is predcitve of change in seats after midterm election
* To reject the null, we use the following test statistic

$$ T = \frac{\text{estimate} - \text{null}}{SE}.$$

* This will follow a $t$ distribution with $n-2$ degrees of freedom.  The 2 comes from the linear model having two parameters: $\beta_0$, intercept, and $\beta_1$, slope.  

* In this course we will not teach how to compute SE - just use the R table output.

* We can thus compute $T = \frac{-0.890-0}{0.835 } = -1.0659$. 
]

---

.pull-left[
#### P value for vote vs. unemployment
```{r}
not_gd_data <- midterms_house %>% 
  filter(! ( year %in% c(1935, 1939)))
nrow(not_gd_data)
lm(house_change ~ unemp, data = not_gd_data) %>% tidy()
```

* We can frame this as hypothesis test:
  - $H_0:\ \beta_1 = 0$, true linear model has slope zero
  - $H_A:\ \beta_1 \neq 0$, true linear model has slope different than zero; unmployment rate is predcitve of change in seats after midterm election
]

--

.pull-right[
* To reject the null, we use the following test statistic

$$ T = \frac{\text{estimate} - \text{null}}{SE} = -1.0659.$$
* We can use R to compute a p-value for this (although lm() does it already) - we have t distribution, we are testing two-sided alternative $(\beta_1 \neq 0)$, so we need to double the left-side area 
```{r}
2* pt(-1.0659, df = nrow(not_gd_data) -2)
```


]


---
### Practice 
* IMS 22.12(a,b)
* IMS 22.13(a,b)
* IMS 24.3(a,b)

* Take 3 minutes to think about the problem, discuss with your neighbors
* I'll call on one of you and ask what your thoughts are for how to approach the problem
* Not expecting you to have the right answer!  Just want to hear how you approach the problem, then help guide you through thinking about the next steps!