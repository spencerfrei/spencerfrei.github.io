---
title: "Functions 1"
subtitle: "<br><br> STA35B: Statistical Data Science 2"
author: "Spencer Frei"
output:
  xaringan::moon_reader:
    # lib_dir: libs
    nature:
      ratio: "16:9"
      # highlightLines: true
      # highlightStyle: solarized-light
      countIncrementalSlides: false
---
  


  
```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(unvotes)
library(knitr)
library(broom)
library(patchwork)
library(ggpubr)
library(scales) # label_dollar 
library(quantreg) # rq
library(kableExtra)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

knitr::opts_chunk$set(comment = NA) # makes it so the ## doesnt appear in output for chunks

source("../_common.R")
```


### Iteration
* We've already seen a few ways to do computations repeatedly in a clean / easy way:
  - `facet_grid()` and `facet_wrap()`, drawing plots for each group
  - `group_by()` + `summarize()` for summarys tatistics for each group
  - creating custom functions
  
* We'll see what other ways R allows for us to flexibly do calculations and save time

---

###  Modifying multiple columns

.pull-left[
* Consider a simple tibble (`runif(n)`: n independent standard normals), and that we want to compute median of every column:
```{r}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)
df |> summarize(
  n = n(),
  a = median(a),
  b = median(b),
  c = median(c),
  d = median(d),
)
```

]

.pull-right[
* Should never be copying+pasting more than twice (what if we had 500 columns!)
* Helpful function: `across()`:
```{r}
df %>%
  summarize(
    n = n(),
    across(a:d, median)
  )
```
* In coming slides, we'll see `across` works and how to modify this behavior.
* Three especially important arguments to `across()`:
  - `.cols`: which columns to iterate over
  - `.fns`: what to do (function) ofr each column
  - `.names`: name output of each column
]

---

### `across()`: Selecting columns with `.cols`

.pull-left[
* For `.cols`, we can use same things we used for `select()`:
```{r}
df %>% summarize(across(-a, median))
df %>% summarize(across(c(a,c), median))
```
* Two additional arguments which are helpful: `everything()` and `where()`. 
```{r}
df %>% summarize(across(everything(), median))
```

]
.pull-right[
* `everything()` computes summaries for every non-grouping variable
* `where()` allows for selecting columns based on type, e.g. `where(is.numeric)` for numbers, `where(is.character)` for strings, `where(is.logical)` for logicals, etc.
```{r}
df <- tibble(
  grp = sample(2, 10, replace = TRUE), # either 1 or 2
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10))
df |> 
  group_by(grp) |> 
  summarize(across(everything(), median))
```

]

---

### `across()`: calling a single function
* `.fns` says how we want data to be transformed
* We are passing the *function* to `across()`, we are not calling the function itself. 
  - Never add the `()` after the function when you pass to across, otherwise you get an error.
```{r, eval=FALSE}
df |> 
  group_by(grp) |> 
  summarize(across(everything(), median()))
#> Error in `summarize()`:
#> ℹ In argument: `across(everything(), median())`.
#> Caused by error in `median.default()`:
#> ! argument "x" is missing, with no default
```
* Same reason why calling `median()` in console will result in an error, since it has no input.

---
### `across()`: calling multiple functions 
.pull-left[ 
* We may want to apply multiple transformations or have multiple arguments
* Motivating example: tibble with missing data
```{r}
rnorm_na <- function(n, n_na, mean = 0, sd = 1) {
  sample(c(rnorm(n - n_na, mean = mean, sd = sd), rep(NA, n_na)))
}
df_miss <- tibble(
  a = rnorm_na(5, 1),
  b = rnorm_na(5, 1),
  c = rnorm_na(5, 2),
  d = rnorm(5))

df_miss |> 
  summarize(
    across(a:d, median),
    n = n())
```

]
.pull-right[
* If we want to pass along argument `na.rm = TRUE` we can create a new function in-line which calls median:
```{r}
df_miss |> 
  summarize(
    across(a:d, function(x) median(x, na.rm = TRUE)),
    n = n()  )
```
* R also allows for a shortcut for in-line function creations: `\`:
```{r, eval=FALSE}
df_miss |> 
  summarize(
    across(a:d, \(x) median(x, na.rm = TRUE)),
    n = n()  )
```
* Equivalent to:
```{r, eval=FALSE}
df_miss |> 
  summarize(
    a = median(a, na.rm = TRUE),
    b = median(b, na.rm = TRUE),
    c = median(c, na.rm = TRUE),
    d = median(d, na.rm = TRUE),
    n = n()   )
```

]
---
.pull-left[ 
* So we can simplify code like ...
```{r}
df_miss |> 
  summarize(
    a = median(a, na.rm = TRUE),
    b = median(b, na.rm = TRUE),
    c = median(c, na.rm = TRUE),
    d = median(d, na.rm = TRUE),
    n = n()   )
```
* ... to ...
```{r}
df_miss |> 
  summarize(
    across(a:d, \(x) median(x, na.rm = TRUE)),
    n = n() )
```
]

.pull-right[
* When we remove missing values, may also be interested in how many were removed.  We can do that again using `across()` by using a named list to `.fns` argument:
```{r}
df_miss |> 
  summarize(
    across(a:d, list(
      median = \(x) median(x, na.rm = TRUE),
      n_miss = \(x) sum(is.na(x))
    )),
    n = n()
  )
```
* Columns are named using "glue": `{.col}.{.fn}`, `.col` is name of original column and `.fn` is name of function. 
* Next: more on how to name columns in the output
]

---

### Column names
* Specifying the `.names` column allows for custom output names:
```{r}
df_miss |> 
  summarize(
    across(
      a:d,
      list(
        median = \(x) median(x, na.rm = TRUE),
        n_miss = \(x) sum(is.na(x))
      ),
      .names = "{.fn}_for_{.col}"
    ),
    n = n(),
  )
```

---

### Column names
* Specifying `.names` is especially important when using `mutate()`, since by default `across()` gives same names as input and thus will replace the original columns.

.pull-left[
* e.g., `coalesce(x, y)` replaces all appearances of `NA` in `x` with the value `y`
```{r}
df_miss |> 
  mutate(
    across(a:d, \(x) coalesce(x, 0))
  )
```


]

.pull-right[
* If we wanted to create new columns, use `.names` to give output new names:
```{r}
df_miss |> 
  mutate(
    across(a:d, \(x) coalesce(x, 0), .names = "{.col}_na_zero")
  )
```

]

---

### Filtering
* `across()` is great with `summarize()` and `mutate()`, but not so much with `filter()` because there we usually combine conditions with `&` / `|`.  
* dplyr provides two variants: `if_any()` and `if_all()` to help combine logicals across columns
```{r}
# same as df_miss |> filter(is.na(a) | is.na(b) | is.na(c) | is.na(d))
df_miss |> filter(if_any(a:d, is.na))

# same as df_miss |> filter(is.na(a) & is.na(b) & is.na(c) & is.na(d))
df_miss |> filter(if_all(a:d, is.na))
```

---

### `across()` in functions
* Let's see an example of expanding all date columns into year / month / day columns.
```{r}
expand_dates <- function(df) {
  df |> 
    mutate(
      across(where(is.Date), list(year = year, month = month, day = mday))
    )
}

df_date <- tibble(
  name = c("Amy", "Bob"),
  date = ymd(c("2009-08-03", "2010-01-16"))
)

df_date |> 
  expand_dates()
```

---
### `across()` in functions
.pull-left[ 
* You can supply multiple columns in a single argument using `c()` in addition to `where()`:
```{r}
summarize_means <- function(df, summary_vars = where(is.numeric)) {
  df |> 
    summarize(
      across({{ summary_vars }}, \(x) mean(x, na.rm = TRUE)),
      n = n(),
      .groups = "drop")
}
```
]
.pull-right[
```{r}
diamonds |> 
  group_by(cut) |> 
  summarize_means()

diamonds |> 
  group_by(cut) |> 
  summarize_means(c(carat, x:z))
```

]

---

### `across()` vs `pivot_longer()`
.pull-left[
Consider calculating medians/means for all columns:
```{r}
df |> 
  summarize(across(a:d, list(median = median, mean = mean)))
```

Alternative way to compute: pivot longer, then group by and summarize:
```{r}
long <- df |> 
  pivot_longer(cols = a:d, names_to = "name", values_to = "value") |>
  group_by(name) |> 
  summarize(
    median = median(value),
    mean = mean(value)
  )
```
]

.pull-right[
```{r,echo=FALSE}
long
```

* Then pivot wider:
```{r}
long %>% pivot_wider(
  names_from = 'name',
  values_from = c(median, mean),
  names_vary = "slowest",
  names_glue = "{name}_{.value}")
```
* "fastest" varies names_from values fastest, resulting in ⁠value1_name1, value1_name2...

]

---
.pull-left[
* This approach is useful when you have groups of columns that you want to compute with simultaneously
* e.g. suppose df contains both values and weights, and we want to compute a weighted mean. 
```{r}
df_paired <- tibble(
  a_val = rnorm(10),
  a_wts = runif(10),
  b_val = rnorm(10),
  b_wts = runif(10),
  c_val = rnorm(10),
  c_wts = runif(10),
  d_val = rnorm(10),
  d_wts = runif(10)
)
```
* No way to do with `across`, but easy with `pivot_longer`
]
.pull-right[
```{r}
( df_long <- df_paired |> 
  pivot_longer(
    cols = everything(), 
    names_to = c("group", ".value"), 
    names_sep = "_"
  ) )
df_long |> 
  group_by(group) |> 
  summarize(mean = weighted.mean(val, wts))
```

]

---

### Examples
.pull-left[ 
* Number of unique values in each column of `palmerpenguins::penguins`:
```{r, echo=FALSE}
library(palmerpenguins)
```
```{r}
penguins %>% 
  summarize(across(everything(), \(x) length(unique(x))))
```
* The mean of every column in `mtcars`:
```{r}
mtcars %>%
  summarize(across(everything(), mean))
```
]

.pull-right[
* Group diamonds by `cut`, `clarity`, and `color`, then count the number of observations and compute the mean of each numeric column.
```{r}
diamonds %>%
  group_by(cut, clarity, color) %>%
  summarize(num = n(), across(where(is.numeric), mean))
```

]


---

### Coming lectures
* Done with R4DS book now; moving to Introduction to Modern Statistics (IMS). 
* We'll use `library(openintro)`.
```{r,echo=FALSE,message=FALSE}
library(openintro)
```

* In the coming lectures, we'll look at **linear models** and how to perform **inference**
* A quick refresher on plotting lines:
  * `y = m x + b`: slope of `m`, intercept of `b` (when `x=0`, `y=b`)
  * `y - v = m (x - u)`: point slope form.  Line going through `(u, v)` with slope `m`
  * If you have two points, you can always connect a line through them and find the slope by substituting into first equation above.

---

### Basic single-variable linear model
* The basic structure of a linear model:
$$ y = b_0 + b_1 x + \epsilon $$
* $b_0$ is **intercept**, $b_1$ is **slope**, $\epsilon$ is an **error term**
* We call $x$ the **predictor**, $y$ the **response** or **outcome**.
* The error term can be small or large - its size relative to the slope determines how useful the linear model is 

```{r, echo=FALSE, message=FALSE}
#| label: fig-imperfLinearModel
#| fig-cap: |
#|   Three datasets where a linear model may be useful even though the data do
#|   not all fall exactly on the line.
#| fig-alt: Three scatterplots with fabricated data.  The first panel shows a 
#|   strong negative linear relationship.  The second panel shows a moderate 
#|   positive linear relationship.  The last panel shows no relationship between 
#|   the x and y variables.
#| fig-width: 4

neg <- simulated_scatter |> filter(group == 1)
pos <- simulated_scatter |> filter(group == 2)
ran <- simulated_scatter |> filter(group == 3)

p_neg <- ggplot(neg, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = NULL, y = NULL)

p_pos <- ggplot(pos, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = NULL, y = NULL)

p_ran <- ggplot(ran, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = NULL, y = NULL)

p_neg
p_pos
p_ran
```

---
### Linear regression for predicting possum head lengths
.pull-left[ 
* We'll look into a dataset which has measurements of possum body lengths and head lengths
* We want to see if we can predict the head length given the body length
* Let's examine a scatterplot of this data (`possum` is in `openintro` library)
```{r}
possum
```
]
.pull-right[
* Let's visualize the relationship between total length and head length:

```{r}
#| label: fig-scattHeadLTotalL
#| fig-cap: |
#|   A scatterplot showing head length against total length for 104 brushtail
#|   possums. A point representing a possum with head length 86.7 mm and total 
#|   length 84 cm is highlighted.
#| fig-alt: A scatterplot with total length on the x-axis and head length on the 
#|   y-axis.  The variables show a moderately strong positive linear relationship.
#|   A single observation is circled in red with coordinates of approximately 84cm 
#|   of total length and 87mm of head length.
ggplot(possum, aes(x = total_l, y = head_l)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(
    x = "Total Length (cm)",
    y = "Head Length (mm)"
  )
```
]

---
.pull-left[
* We can try and fit a line which roughly fits the data well by taking two points and then drawing the line between them.  
```{r}
ggplot(possum, aes(x = total_l, y = head_l)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(
    x = "Total Length (cm)",
    y = "Head Length (mm)"
  ) +
  geom_point(data = sample_n(possum, 2), aes(x = total_l, y = head_l), color = "red", size = 5, shape = "circle open", stroke = 2)
```
* Not ideal
]

.pull-right[
* A more reasonable line:

```{r}
#| 
ggplot(possum, aes(x = total_l, y = head_l)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(
    x = "Total Length (cm)",
    y = "Head Length (mm)"
  ) +
  geom_smooth(method = "lm", se = FALSE)
```
]


---
### Residuals
* Suppose we have data $(x_i, y_i)_{i=1}^n$, and we fit a line using the data, and we make a prediction $\hat{y}(x_i) = \hat y_i$.  
* The **residual** for the $i$-th observation $(x_i, y_i)$ is the difference between observed outcome and predicted outcome:
$$ e_i := y_i - \hat y_i .$$

.pull-left[ 
```{r, echo=FALSE, message=FALSE}
mod <- lm(head_l ~ total_l, data = possum)
preds <- predict(mod, data.frame(total_l = c(76, 85, 95.5)))
obs <- c(85.1, 98.6, 94)
ggplot(possum, aes(x = total_l, y = head_l)) +
  geom_point(alpha = 0.8, size = 2) +
  labs(
    x = "Total Length (cm)",
    y = "Head Length (mm)"
  ) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(
    data = possum |> filter(total_l == 76),
    shape = "circle open", stroke = 2,
    size = 4, color = IMSCOL["red", "full"]
  ) +
  geom_segment(aes(x = 76, y = preds[1], xend = 76, yend = obs[1] + 0.4),
    color = IMSCOL["red", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = possum |> filter(total_l == 85, head_l == 98.6),
    shape = "diamond open", stroke = 2, size = 5,
    color = IMSCOL["gray", "full"]
  ) +
  geom_segment(aes(x = 85, y = preds[2], xend = 85, yend = obs[2] - 0.5),
    color = IMSCOL["gray", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = possum |> filter(total_l == 95.5, head_l == 94),
    shape = "triangle open", stroke = 2, size = 5,
    color = IMSCOL["pink", "full"]
  ) +
  geom_segment(aes(x = 95.5, y = preds[3], xend = 95.5, yend = obs[3] + 0.5),
    color = IMSCOL["pink", "full"], inherit.aes = FALSE
  )
```

]

.pull-right[
* We plot a linear model on the left for getting possum head length vs. possum body length.  
* The length of the lines from the black line to the circled points is the residual; if the dot is above the line, it is positive, if it is below the line, it is negative.
* Suppose the line is given by the equation
$$ \hat y = 41 + 0.59 x $$
* What is the residual for the observation (76.0, 85.1)?
$$ \hat y = 41 + 0.59 \times 85.1 = 85.84 $$
$$ e = y - \hat y = 85.1 - 85.84 = -0.74 $$
]

---

### Residual plots

* Residuals are often helpfuul when trying to understand how well a linear model fits the data
* We can visualize this by plotting predicted values on the $x$ axis, and residuals on the $y$-axis
.pull-left[ 
```{r, echo=FALSE, message=FALSE}
mod <- lm(head_l ~ total_l, data = possum)
preds <- predict(mod, data.frame(total_l = c(76, 85, 95.5)))
obs <- c(85.1, 98.6, 94)
ggplot(possum, aes(x = total_l, y = head_l)) +
  geom_point(alpha = 0.8, size = 2) +
  labs(
    x = "Total Length (cm)",
    y = "Head Length (mm)"
  ) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(
    data = possum |> filter(total_l == 76),
    shape = "circle open", stroke = 2,
    size = 4, color = IMSCOL["red", "full"]
  ) +
  geom_segment(aes(x = 76, y = preds[1], xend = 76, yend = obs[1] + 0.4),
    color = IMSCOL["red", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = possum |> filter(total_l == 85, head_l == 98.6),
    shape = "diamond open", stroke = 2, size = 5,
    color = IMSCOL["gray", "full"]
  ) +
  geom_segment(aes(x = 85, y = preds[2], xend = 85, yend = obs[2] - 0.5),
    color = IMSCOL["gray", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = possum |> filter(total_l == 95.5, head_l == 94),
    shape = "triangle open", stroke = 2, size = 5,
    color = IMSCOL["pink", "full"]
  ) +
  geom_segment(aes(x = 95.5, y = preds[3], xend = 95.5, yend = obs[3] + 0.5),
    color = IMSCOL["pink", "full"], inherit.aes = FALSE
  )
```
]

.pull-right[
* Residual plot:
```{r}
#| echo: false
#| message: false

m_head_total <- lm(head_l ~ total_l, data = possum)
m_head_total_aug <- augment(m_head_total)

ggplot(m_head_total_aug, aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.8, size = 2) +
  labs(
    x = "Predicted values of head length (mm)",
    y = "Residuals"
  ) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_point(
    data = m_head_total_aug |>
      filter(total_l == 76),
    shape = "circle open", stroke = 2, size = 4,
    color = IMSCOL["red", "full"]
  ) +
  geom_segment(
    aes(
      x = preds[1], y = obs[1] - preds[1] + 0.2,
      xend = preds[1], yend = 0
    ),
    color = IMSCOL["red", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = m_head_total_aug |>
      filter(total_l == 85, head_l == 98.6),
    shape = "diamond open", stroke = 2, size = 5,
    color = IMSCOL["gray", "full"]
  ) +
  geom_segment(
    aes(
      x = preds[2], y = obs[2] - preds[2] - 0.3,
      xend = preds[2], yend = 0
    ),
    color = IMSCOL["gray", "full"], inherit.aes = FALSE
  ) +
  geom_point(
    data = m_head_total_aug |>
      filter(total_l == 95.5, head_l == 94),
    shape = "triangle open", stroke = 2, size = 5,
    color = IMSCOL["pink", "full"]
  ) +
  geom_segment(
    aes(
      x = preds[3], y = obs[3] - preds[3] + 0.3,
      xend = preds[3], yend = 0
    ),
    color = IMSCOL["pink", "full"], inherit.aes = FALSE
  )
```

]

---

### Residual plots

.pull-left[ 
* Let's look at a few more linear relationships and their corresponding residual plots.

```{r}
#| echo: false
#| message: false
#| warning: false

neg_lin <- simulated_scatter |> filter(group == 6)
neg_cur <- simulated_scatter |> filter(group == 7)
random <- simulated_scatter |> filter(group == 8)

neg_lin_mod <- augment(lm(y ~ x, data = neg_lin))
neg_cur_mod <- augment(lm(y ~ x, data = neg_cur))
random_mod <- augment(lm(y ~ x, data = random))

p_neg_lin <- ggplot(neg_lin, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_neg_cur <- ggplot(neg_cur, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_random <- ggplot(random, aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_neg_lin_res <- ggplot(neg_lin_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_neg_cur_res <- ggplot(neg_cur_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_random_res <- ggplot(random_mod, aes(x = .fitted, y = .resid)) +
  geom_point(size = 2, alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_void() +
  theme(panel.border = element_rect(colour = "gray", fill = NA, size = 1))

p_neg_lin + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) +
  p_neg_cur + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_random +
  p_neg_lin_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) +
  p_neg_cur_res + theme(plot.margin = unit(c(0, 10, 5, 0), "pt")) + p_random_res +
  plot_layout(ncol = 3, heights = c(2, 1))
```

* What are some observed trends?
]

--

.pull-right[ 
* First dataset, no obvious patterns for residuals: they look random around the dashed line
* Second dataset: looks like some curvature.  Suggests the errors are *not* random, true relationship likely not linear plus noise.
* Third plot: not clear if underlying relatinoship is linear, but residuals don't appear to show any pattern. 
]


---

### Correlation for describing linear relationships

.pull-left[ 
* **Correlation** defines the strength of a linear relationship
* Always valued between -1 and 1, denoted by $r$
* Correlation for observations $(x_1, y_1), \dots, (x_n, y_n)$:
$$ r  = \frac{1}{n-1} \sum_{i=1}^n \frac{x_i - \bar x}{s_x} \cdot \frac{y_i - \bar y}{s_y}, $$
where $\bar x, \bar y$ sample means and $s_x, s_y$ standard deviations for $x_i, y_i$ respectively. 
* Correlation of +1: positive linear relationship; -1: negative; 0: not linear relationship. 
]

.pull-right[ 
```{r}
#| echo: false
#| message: false
#| warning: false 

# library(ggpubr) # Adding here instead of _common.R to avoid collision with ggimage

simulated_scatter |>
  filter(group %in% c(9:12, 14:17)) |>
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2, alpha = 0.8) +
  theme_void() +
  facet_wrap(~group, nrow = 2, scales = "free") +
  theme(
    panel.border = element_rect(colour = "gray", fill = NA, linewidth = 1),
    strip.background = element_blank(),
    strip.text.x = element_blank()
  ) +
  ggpubr::stat_cor(aes(label = paste("r", ..r.., sep = "~`=`~")))
```
]
---

### Correlation is "scale-invariant"
.pull-left[ 
* The definition of correlation:
$$ r  = \frac{1}{n-1} \sum_{i=1}^n \frac{x_i - \bar x}{s_x} \cdot \frac{y_i - \bar y}{s_y}, $$
* If we multiply every $x_i$ by a number $\epsilon$, then both the numerator and denominator scale by the same number $\epsilon$, so $r$ is unaffected
  * Same thing for $y_i$'s
* E.g. correlation between height/weight using metric or American units is the same
]

.pull-right[ 
```{r}
#| label: fig-bdims-units
#| echo: false
#| message: false
#| warning: false
#| fig-width: 6

p_1 <- ggplot(bdims, aes(x = hgt, y = wgt)) +
  geom_point(alpha = 0.8) +
  labs(x = "Height (cm)", y = "Weight (kg)") +
  stat_cor(aes(label = paste("r", ..r.., sep = "~`=`~")))

p_2 <- bdims |>
  mutate(
    hgt = hgt * 0.393701,
    wgt = wgt * 2.20462
  ) |>
  ggplot(aes(x = hgt, y = wgt)) +
  geom_point(alpha = 0.8) +
  labs(x = "Height (in)", y = "Weight (lbs)") +
  stat_cor(aes(label = paste("r", ..r.., sep = "~`=`~")))

p_1 + p_2 +
  plot_annotation(tag_levels = "A")
```
]

---
### Least squares regression
.pull-left[
* How do we define the "best" line of fit?
* We would like the residuals to be small, but many ways to measure residuals.
* For residuals $e_i = y_i - \hat y_i$, consider these two ways:
$$ |e_1| + |e_2| + \dots + |e_n|$$
$$ e_1^2 + e_2^2 + \dots + e_n^2$$
* We plot the two lines which minimizes either the sum of the absolute values or the sum of the squares on the right
]

.pull-right[

```{r}
#| echo: false
#| message: false
#| warning: false

ggplot(elmhurst, aes(x = family_income, y = gift_aid)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = "lm", se = FALSE, aes(linetype = "Least squares"), color = "blue") + 
  geom_smooth(method = rq, formula = y ~ x, se = FALSE, aes(linetype = "Absolute value"), color = "blue") +
  scale_y_continuous(labels = label_dollar(scale = 1, suffix = "K", accuracy = 1)) +
  scale_x_continuous(labels = label_dollar(scale = 1, suffix = "K", accuracy = 1)) +
  labs(x = "Family income", y = "Gift aid from university", linetype = "Method") +
  scale_linetype_manual(values = c("Least squares" = "solid", "Absolute value" = "dashed"))
```
* Most commonly used one is least squares, we'll focus on this 
]

---
### Fitting least squares lines in R
.pull-left[
* Let's consider the `elmhurst` dataset:
```{r}
elmhurst
```
* Suppose we want to predict amoutn of aid given as a functino of family income.  We want to form predictions
$$ \widehat{\mathsf{aid}} = \beta_0 + \beta_1 \times \mathsf{familyincome}$$
]
.pull-right[
* To compute line of best fit of variable $y$ given $x$, we use `lm(y ~ x)`:
```{r}
lm(gift_aid ~ family_income, data = elmhurst)
lm(gift_aid ~ family_income, data = elmhurst) %>% broom::tidy()
```
* `broom` package takes outputs of messy functions like `lm()` and makes them into tidy tibbles 
]

---
.pull-left[
```{r}
lm(gift_aid ~ family_income, data = elmhurst)
lm(gift_aid ~ family_income, data = elmhurst) %>% summary()
```
]
.pull-right[
```{r}
lm(gift_aid ~ family_income, data = elmhurst) %>% tidy()
```
```{r,echo=FALSE,message=FALSE}
m_ga_fi <- lm(gift_aid ~ family_income, data = elmhurst) |> tidy()
m_ga_fi_int <- m_ga_fi |>
  filter(term == "(Intercept)") |>
  pull(estimate)
m_ga_fi_slope <- m_ga_fi |>
  filter(term == "family_income") |>
  pull(estimate)
```
* How do we interpret these numbers?
* Intercept and slope estimates for the Elmhurst data are $b_0$ = `r m_ga_fi_int` and $b_1$ = `r m_ga_fi_slope`.
* Slope: For each additional 1,000 of family income, we expect a student to receive a net difference of $1,000 * (-0.0431) = -43.10$ in aid on average, i.e., 43.10 *less*.
* This is an **association** not **causal**: increasing a student family's income does nto necessarily cause student aid to drop, just likely
* Intercept:  $b_0$ = `r m_ga_fi_int` describes the average aid if a student's family had no income, `r format(round(m_ga_fi_int*1000,0), scientific = FALSE, big.mark = ",")`
* Intercept here is interpretable, but not always
* p-values provide insight into how "significant" the relationship between variables and response are.  We'll discuss more later. 
]

---

#### Computing the line of best fit from mean and standard deviations
.pull-left[ 
* Two crucial facts for computing line of best fit:
* (1) Slope of least squares line can be computed by
$$ b_1 = \frac{s_y}{s_x} \cdot r,$$
where $r$ is correlation, $s_x, s_y$ are sample standard deviations of predictor and outcome, respectively
* (2) If $\bar x$ is sample mean of predictor, $\bar y$ sample mean of outcome, then $(\bar x, \bar y)$ is on the least squares line


```{r}
#| label: tbl-summaryStatsElmhurstRegr
#| tbl-cap: Summary statistics for family income and gift aid.
#| echo: false
elmhurst |>
  select(family_income, gift_aid) |>
  summarise(
    fi_m = mean(family_income),
    fi_s = sd(family_income),
    ga_m = mean(gift_aid),
    ga_s = sd(gift_aid),
    r    = cor(family_income, gift_aid)
  ) |>
  kbl(
    linesep = "", booktabs = TRUE,
    col.names = c("mean", "sd", "mean", "sd", "r"),
    align = "ccccc"
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    latex_options = c("striped", "hold_position"), full_width = FALSE
  ) |>
  add_header_above(c("Family income, x" = 2, "Gift aid, y" = 2, " " = 1))
```
]

.pull-right[
* Compute slope of least squares line:
$$ b_1 = \frac{s_y}{s_x} \cdot r = \frac{5.46}{63.2} \cdot (-0.499) = -0.0431$$
* Point slope form of a line: for slope $m$ and point $(x_0, y_0)$ on a line,
$$ y - y_0 = m \cdot (x - x_0) $$
* Since $(\bar x, \bar y)$ is on the line with slope $b_1$, we get
$$ y - \bar y = b_1 (x - \bar y)$$
* This gives
$$ y = \bar y - b_1 \bar x + b_1 x \implies $$
$$ b_0 = \bar y - b_1 \bar x = 19.9 + 0.0431 * 102 = 24.3. $$
* Results in equation:
$$ \widehat {\mathsf{aid}} = 24.3 - 0.0431 \times \mathsf{familyincome} $$
]

---
* Let's compare our calculated line of best fit with what R returns to us with `lm()`:
$$ \widehat {\mathsf{aid}} = 24.3 - 0.0431 \times \mathsf{familyincome} $$

```{r}
lm(gift_aid ~ family_income, data = elmhurst) %>% tidy()
```

* If we have a senior who is considering Elmhurst College, can they use this to predict her financial aid?
* They may use it as an *estimate*.  Things can vary year to year, and the estimate is imperfect.  So likely will be errors. 


---
### Extrapolation and its perils
.pull-left[ 
* We can make some informed estimates about future predictions of a linear model when the predictors are similar to those we saw before.
* Consider if we had data which looked like the left, and we tried to make predictions for predictors which are outside of the domain where we have data
]
.pull-right[ 
```{r, echo = FALSE, warning= FALSE, message= FALSE}
library(ggplot2)
library(gridExtra) # or library(patchwork)

# Generate linear data
set.seed(123) # for reproducibility
x1 <- 1:10
y1 <- x1 + rnorm(length(x1), mean = 0, sd = 0.3)
linear_data <- data.frame(x = x1, y = y1)

# Generate quadratic data
x2 <- 10:15
# Quadratic equation adjusted for a decrease after the peak
y2 <- -2*(x2 - 10)^2 + 10 + rnorm(length(x2), mean = 0, sd = 0.3)
quadratic_data <- data.frame(x = c(x1, x2), y = c(y1, y2))

# Plot 1: Linear data with line of best fit
p1 <- ggplot(linear_data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Linear Data with Best Fit Line")

# Plot 2: Combined data with quadratic trend and y=x line
p2 <- ggplot(quadratic_data, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = x), color = "red") +
  ggtitle("Extrapolation for future data")

p1 + p2 
```
]
